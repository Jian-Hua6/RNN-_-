{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# RNN聊天機器人 - 混合回應系統\n",
        "\n",
        "基於PyTorch的序列到序列模型，結合智能混合回應系統。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f75398-6567-47e8-e4e9-94ffc2996c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:44: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#&\\._%\\-]+)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/__init__.py:46: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  re_skip_default = re.compile(\"(\\r\\n|\\s)\", re.U)\n",
            "/usr/local/lib/python3.12/dist-packages/jieba/finalseg/__init__.py:78: SyntaxWarning: invalid escape sequence '\\.'\n",
            "  re_skip = re.compile(\"([a-zA-Z0-9]+(?:\\.\\d+)?%?)\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用設備: cuda\n",
            "環境設置完成\n"
          ]
        }
      ],
      "source": [
        "# 環境設置\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import re\n",
        "import itertools\n",
        "import numpy as np\n",
        "import jieba\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 檢查GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用設備: {device}\")\n",
        "\n",
        "# 設置隨機種子\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "print(\"環境設置完成\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88fefa5e-13c5-49fa-b433-ba781dfc5825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jieba in /usr/local/lib/python3.12/dist-packages (0.42.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package fonts-noto-cjk.\n",
            "(Reading database ... 126666 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-noto-cjk_1%3a20220127+repack1-1_all.deb ...\n",
            "Unpacking fonts-noto-cjk (1:20220127+repack1-1) ...\n",
            "Setting up fonts-noto-cjk (1:20220127+repack1-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "依賴安裝完成\n"
          ]
        }
      ],
      "source": [
        "# 安裝依賴\n",
        "!pip install jieba matplotlib tqdm\n",
        "\n",
        "# 設置中文字體\n",
        "!apt-get update -qq\n",
        "!apt-get install -y fonts-noto-cjk -qq\n",
        "\n",
        "plt.rcParams['font.family'] = ['Noto Sans CJK TC', 'DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "print(\"依賴安裝完成\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff1d43d6-c834-482d-90f5-f0dcc6cb07cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "配置完成\n",
            "  iterations: 800\n",
            "  batch_size: 8\n",
            "  learning_rate: 0.0001\n",
            "  hidden_size: 64\n",
            "  n_layers: 1\n",
            "  dropout: 0.1\n",
            "  print_every: 50\n",
            "  clip: 50.0\n",
            "  max_length: 6\n",
            "  confidence_threshold: 0.3\n"
          ]
        }
      ],
      "source": [
        "# 配置參數\n",
        "CONFIG = {\n",
        "    'iterations': 800,\n",
        "    'batch_size': 8,\n",
        "    'learning_rate': 0.0001,\n",
        "    'hidden_size': 64,\n",
        "    'n_layers': 1,\n",
        "    'dropout': 0.1,\n",
        "    'print_every': 50,\n",
        "    'clip': 50.0,\n",
        "    'max_length': 6,\n",
        "    'confidence_threshold': 0.3\n",
        "}\n",
        "\n",
        "print(\"配置完成\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9334f8bc-177d-46ef-a887-a7bb0d57f0cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.837 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.837 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "詞彙表大小: 37\n",
            "訓練對數: 11\n"
          ]
        }
      ],
      "source": [
        "# 數據和詞彙表\n",
        "PAD_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "        self.UNK_token = None\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        words = list(jieba.cut(sentence)) if re.search(r'[\\u4e00-\\u9fff]', sentence) else sentence.split()\n",
        "        for word in words:\n",
        "            if word.strip():\n",
        "                self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def add_unknown_token(self):\n",
        "        if \"UNK\" not in self.word2index:\n",
        "            self.UNK_token = self.num_words\n",
        "            self.word2index[\"UNK\"] = self.num_words\n",
        "            self.index2word[self.num_words] = \"UNK\"\n",
        "            self.num_words += 1\n",
        "\n",
        "# 訓練數據\n",
        "sample_data = [\n",
        "    [\"你好\", \"你好\"],\n",
        "    [\"hi\", \"Hello\"],\n",
        "    [\"什麼是RNN\", \"循環神經網絡\"],\n",
        "    [\"LSTM是什麼\", \"長短期記憶\"],\n",
        "    [\"GRU特點\", \"門控循環單元\"],\n",
        "    [\"什麼是AI\", \"人工智能技術\"],\n",
        "    [\"人工智能\", \"模擬人類智能\"],\n",
        "    [\"機器學習\", \"自動學習方法\"],\n",
        "    [\"深度學習\", \"多層神經網絡\"],\n",
        "    [\"謝謝\", \"不客氣\"],\n",
        "    [\"再見\", \"再見\"]\n",
        "]\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = str(s).lower().strip()\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1 \", s)\n",
        "    return re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "\n",
        "# 構建詞彙表\n",
        "voc = Voc(\"chatbot\")\n",
        "pairs = []\n",
        "\n",
        "for q, a in sample_data:\n",
        "    q = normalizeString(q)\n",
        "    a = normalizeString(a)\n",
        "    voc.addSentence(q)\n",
        "    voc.addSentence(a)\n",
        "    pairs.append([q, a])\n",
        "\n",
        "voc.add_unknown_token()\n",
        "print(f\"詞彙表大小: {voc.num_words}\")\n",
        "print(f\"訓練對數: {len(pairs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "models",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88f7d9d2-c645-4b74-c9f5-becfa3f7602d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型定義完成\n"
          ]
        }
      ],
      "source": [
        "# 簡化的模型定義\n",
        "class SimpleEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(SimpleEncoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        embedded = self.embedding(input_seq)\n",
        "        output, hidden = self.gru(embedded)\n",
        "        return output, hidden\n",
        "\n",
        "class SimpleDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(SimpleDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_step, hidden):\n",
        "        embedded = self.embedding(input_step)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output = self.out(output)\n",
        "        return F.softmax(output, dim=2), hidden\n",
        "\n",
        "print(\"模型定義完成\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hybrid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ea99e0-a0f0-4de1-b2c7-8440c23c84a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "混合系統初始化完成\n"
          ]
        }
      ],
      "source": [
        "# 混合回應系統\n",
        "class HybridSystem:\n",
        "    def __init__(self):\n",
        "        self.responses = {\n",
        "            r'你好|hi|hello': [\"您好！我是AI助手。\"],\n",
        "            r'rnn|循環': [\"RNN是處理序列數據的神經網絡。\"],\n",
        "            r'lstm|長短期': [\"LSTM解決了梯度消失問題。\"],\n",
        "            r'gru|門控': [\"GRU是LSTM的簡化版本。\"],\n",
        "            r'ai|人工智能|人工智慧|什麼是ai': [\"AI（人工智能）是讓機器模擬人類智能的技術，包括學習、推理、感知等能力。\"],\n",
        "            r'機器學習|machine learning': [\"機器學習是AI的一個分支，讓機器從數據中自動學習和改進。\"],\n",
        "            r'深度學習|deep learning': [\"深度學習使用多層神經網絡來模擬人腦的學習過程。\"],\n",
        "            r'謝謝|thank': [\"不客氣！\"],\n",
        "            r'再見|bye': [\"再見！\"]\n",
        "        }\n",
        "\n",
        "    def get_rule_response(self, user_input):\n",
        "        user_input = user_input.lower()\n",
        "        for pattern, responses in self.responses.items():\n",
        "            if re.search(pattern, user_input, re.IGNORECASE):\n",
        "                return random.choice(responses)\n",
        "        return \"這是一個有趣的問題。\"\n",
        "\n",
        "    def get_response(self, user_input, rnn_response=None):\n",
        "        if rnn_response and len(rnn_response.strip()) > 2:\n",
        "            return rnn_response\n",
        "        return self.get_rule_response(user_input)\n",
        "\n",
        "hybrid_system = HybridSystem()\n",
        "print(\"混合系統初始化完成\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9cb2761-98d9-4b99-d436-74bc284bec35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "開始訓練...\n",
            "迭代 0, 損失: 7.2160\n",
            "迭代 50, 損失: 14.4192\n",
            "迭代 100, 損失: 14.4162\n",
            "迭代 150, 損失: 7.1508\n",
            "迭代 200, 損失: 14.2855\n",
            "迭代 250, 損失: 14.0396\n",
            "迭代 300, 損失: 13.6954\n",
            "迭代 350, 損失: 13.5993\n",
            "迭代 400, 損失: 13.6348\n",
            "迭代 450, 損失: 13.6211\n",
            "迭代 500, 損失: 6.2851\n",
            "迭代 550, 損失: 6.3339\n",
            "迭代 600, 損失: 13.5605\n",
            "迭代 650, 損失: 13.5958\n",
            "迭代 700, 損失: 9.9422\n",
            "迭代 750, 損失: 13.4824\n",
            "訓練完成！\n"
          ]
        }
      ],
      "source": [
        "# 訓練函數\n",
        "def sentence_to_indexes(voc, sentence):\n",
        "    words = list(jieba.cut(sentence)) if re.search(r'[\\u4e00-\\u9fff]', sentence) else sentence.split()\n",
        "    indexes = []\n",
        "    for word in words:\n",
        "        if word in voc.word2index:\n",
        "            indexes.append(voc.word2index[word])\n",
        "        else:\n",
        "            indexes.append(voc.UNK_token if voc.UNK_token else 0)\n",
        "    return indexes + [EOS_token]\n",
        "\n",
        "def train_model():\n",
        "    hidden_size = CONFIG['hidden_size']\n",
        "\n",
        "    encoder = SimpleEncoder(voc.num_words, hidden_size).to(device)\n",
        "    decoder = SimpleDecoder(voc.num_words, hidden_size).to(device)\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=CONFIG['learning_rate'])\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=CONFIG['learning_rate'])\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"開始訓練...\")\n",
        "\n",
        "    for iteration in range(CONFIG['iterations']):\n",
        "        # 隨機選擇訓練對\n",
        "        pair = random.choice(pairs)\n",
        "        input_sentence, target_sentence = pair\n",
        "\n",
        "        # 轉換為索引\n",
        "        input_indexes = sentence_to_indexes(voc, input_sentence)\n",
        "        target_indexes = sentence_to_indexes(voc, target_sentence)\n",
        "\n",
        "        # 轉換為張量\n",
        "        input_tensor = torch.LongTensor([input_indexes]).to(device)\n",
        "        target_tensor = torch.LongTensor(target_indexes).to(device)\n",
        "\n",
        "        # 清零梯度\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        # 編碼\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor)\n",
        "\n",
        "        # 解碼\n",
        "        decoder_input = torch.LongTensor([[SOS_token]]).to(device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        loss = 0\n",
        "        for i in range(len(target_indexes)):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output.squeeze(0), target_tensor[i:i+1])\n",
        "            decoder_input = target_tensor[i:i+1].unsqueeze(0)\n",
        "\n",
        "        # 反向傳播\n",
        "        loss.backward()\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        if iteration % CONFIG['print_every'] == 0:\n",
        "            print(f\"迭代 {iteration}, 損失: {loss.item():.4f}\")\n",
        "\n",
        "    print(\"訓練完成！\")\n",
        "    return encoder, decoder\n",
        "\n",
        "encoder, decoder = train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48f744ca-f4c1-4db9-9b8e-701826b7776b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "測試結果:\n",
            "輸入: 你好 -> 回應: 您好！我是AI助手。\n",
            "輸入: 什麼是RNN -> 回應: RNN是處理序列數據的神經網絡。\n",
            "輸入: LSTM是什麼 -> 回應: LSTM解決了梯度消失問題。\n",
            "輸入: 什麼是AI -> 回應: AI（人工智能）是讓機器模擬人類智能的技術，包括學習、推理、感知等能力。\n",
            "輸入: 人工智能 -> 回應: AI（人工智能）是讓機器模擬人類智能的技術，包括學習、推理、感知等能力。\n",
            "輸入: 機器學習 -> 回應: 機器學習是AI的一個分支，讓機器從數據中自動學習和改進。\n",
            "輸入: 謝謝 -> 回應: 不客氣！\n",
            "輸入: 再見 -> 回應: 再見！\n"
          ]
        }
      ],
      "source": [
        "# 評估和對話\n",
        "def generate_response(sentence):\n",
        "    try:\n",
        "        sentence = normalizeString(sentence)\n",
        "        input_indexes = sentence_to_indexes(voc, sentence)\n",
        "        input_tensor = torch.LongTensor([input_indexes]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor)\n",
        "\n",
        "            decoder_input = torch.LongTensor([[SOS_token]]).to(device)\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            decoded_words = []\n",
        "            for _ in range(CONFIG['max_length']):\n",
        "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "                topv, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze().detach().unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "                if topi.item() == EOS_token:\n",
        "                    break\n",
        "                elif topi.item() in voc.index2word:\n",
        "                    decoded_words.append(voc.index2word[topi.item()])\n",
        "\n",
        "            rnn_response = \"\".join(decoded_words) if any(re.search(r'[\\u4e00-\\u9fff]', w) for w in decoded_words) else \" \".join(decoded_words)\n",
        "            return hybrid_system.get_response(sentence, rnn_response)\n",
        "\n",
        "    except Exception as e:\n",
        "        return hybrid_system.get_rule_response(sentence)\n",
        "\n",
        "# 測試\n",
        "test_inputs = [\"你好\", \"什麼是RNN\", \"LSTM是什麼\", \"什麼是AI\", \"人工智能\", \"機器學習\", \"謝謝\", \"再見\"]\n",
        "\n",
        "print(\"測試結果:\")\n",
        "for test_input in test_inputs:\n",
        "    response = generate_response(test_input)\n",
        "    print(f\"輸入: {test_input} -> 回應: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0601319-73d2-4491-d0cc-d037fabe65f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "系統準備完成！\n",
            "混合回應系統聊天機器人已準備就緒！\n",
            "輸入 'quit' 結束對話\n",
            "==============================\n",
            "\n",
            "你: 什麼是RNN\n",
            "機器人: RNN是處理序列數據的神經網絡。\n",
            "\n",
            "你: 什麼是LSTM\n",
            "機器人: LSTM解決了梯度消失問題。\n",
            "\n",
            "你: 什麼是AI\n",
            "機器人: AI（人工智能）是讓機器模擬人類智能的技術，包括學習、推理、感知等能力。\n",
            "\n",
            "你: 再見\n",
            "機器人: 再見！\n",
            "\n",
            "機器人: 再見！\n"
          ]
        }
      ],
      "source": [
        "# 互動對話\n",
        "def start_chat():\n",
        "    print(\"混合回應系統聊天機器人已準備就緒！\")\n",
        "    print(\"輸入 'quit' 結束對話\")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"\\n你: \").strip()\n",
        "\n",
        "            if user_input.lower() in ['quit', 'exit', '退出']:\n",
        "                print(\"機器人: 再見！\")\n",
        "                break\n",
        "\n",
        "            if not user_input:\n",
        "                continue\n",
        "\n",
        "            response = generate_response(user_input)\n",
        "            print(f\"機器人: {response}\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n機器人: 再見！\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"機器人: 抱歉，出現問題。\")\n",
        "\n",
        "print(\"系統準備完成！\")\n",
        "start_chat()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}